# rul_model_trainer_all_sets.py (Spearman ê°œì„ : ë‚®ì€ threshold + ìƒìœ„ Nê°œ ì„ íƒ + ë‹¤ì±„ë„)
'''
ì±„ë„ 4ê°œ(CH1 ~ CH4) â†’ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ê³  ê²°í•©í–ˆëŠ”ê°€?

### ê²°í•© ë°©ì‹

ê° ì±„ë„ì—ì„œ ì¶”ì¶œí•œ íŠ¹ì§•ë“¤ì„ **í•˜ë‚˜ì˜ ë²¡í„°**ë¡œ **ë³‘ë ¬ ê²°í•©(concatenate)**í•©ë‹ˆë‹¤.

CH1_mean, CH1_std, CH1_entropy, ..., CH4_band_power, CH4_entropy

â€”>>>>>>ê·¸ë ‡ë‹¤ë©´ ì—”íŠ¸ë¡œí”¼ì˜ ë¹„ì¤‘ì´ ë‚®ìœ¼ë¯€ë¡œ í‚¤ìš°ì(ì—”íŠ¸ë¡œí”¼ê°’ *3)
->>>>>> chë³„ ìƒìœ„ 10ê°œ ì¶”ì¶œ
->>>>>> ì´ˆë‹¨ìœ„ ë¼ë²¨ë§
->>>>>> ê³ ì¥ ì£¼íŒŒìˆ˜ ë°˜ì˜
'''
import os
import numpy as np
import pandas as pd
from glob import glob
from scipy.stats import kurtosis, skew, spearmanr
from scipy.signal import welch
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from nptdms import TdmsFile



# â–¶ Spearman ê¸°ë°˜ ì„ íƒëœ ì£¼íŒŒìˆ˜ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
SELECTED_FREQ_INDICES = {}
FREQ_VECTOR = None

# â–¶ ì§„ë™ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def load_vibration_data(file_path):
    tdms_file = TdmsFile.read(file_path)
    group_name = tdms_file.groups()[0].name  # ì§„ë™ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” ì²« ê·¸ë£¹
    vib_channels = tdms_file[group_name].channels()
    vib_data = {ch.name: ch.data for ch in vib_channels}
    return pd.DataFrame(vib_data)

# â–¶ ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
def extract_timestamp(f):
    name = os.path.basename(f)
    time_part = name.split("_")[-1].replace(".tdms", "")
    return pd.to_datetime(time_part, format="%Y%m%d%H%M%S")

FAULT_FREQS = [140, 93, 78]  # ê³ ì¥ ê´€ë ¨ ì£¼íŒŒìˆ˜ (Hz)

def compute_selected_frequency_indices(file_list, channels, top_n=20, sampling_rate=25600):
    psd_by_channel = {ch: [] for ch in channels}
    rul_list = []

    for df, rul in file_list:
        for ch in channels:
            if ch not in df.columns:
                continue
            data = df[ch].values
            f, Pxx = welch(data, fs=sampling_rate)
            psd_by_channel[ch].append(Pxx)
        rul_list.append(rul)

    selected = {}
    for ch in channels:
        psd_matrix = np.array(psd_by_channel[ch])
        if psd_matrix.shape[0] == 0:
            continue

        rho_list = [abs(spearmanr(psd_matrix[:, i], rul_list)[0]) for i in range(psd_matrix.shape[1])]
        top_indices = np.argsort(rho_list)[-top_n:]
        fault_indices = [np.argmin(np.abs(f - ff)) for ff in FAULT_FREQS]
        total_indices = sorted(set(top_indices.tolist() + fault_indices))
        selected[ch] = total_indices

    print(f"[âœ“] ì´ˆ ë‹¨ìœ„ Spearman+Fault ê¸°ë°˜ ì£¼íŒŒìˆ˜ ì„ íƒ ì™„ë£Œ (ì´ {len(selected[channels[0]])}ê°œ)")
    return selected, f



# â–¶ ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° í•¨ìˆ˜ (ì„ íƒëœ ì£¼íŒŒìˆ˜ ê¸°ë°˜)
def energy_entropy_selected(data, selected_indices, sampling_rate=25600):
    f, Pxx = welch(data, fs=sampling_rate)
    selected = Pxx[selected_indices]
    selected = selected / np.sum(selected)
    selected = selected[selected > 0]
    return -np.sum(selected * np.log(selected))

# â–¶ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_features_from_vibration(vib_df, sampling_rate=25600):
    features = {}
    for ch in vib_df.columns:
        data = vib_df[ch].values
        rms = np.sqrt(np.mean(data**2))
        f, Pxx = welch(data, fs=sampling_rate)

        features[f'{ch}_mean'] = np.mean(data)
        features[f'{ch}_std'] = np.std(data)
        features[f'{ch}_rms'] = rms
        features[f'{ch}_kurtosis'] = kurtosis(data)
        features[f'{ch}_skew'] = skew(data)
        features[f'{ch}_crest'] = np.max(np.abs(data)) / rms
        features[f'{ch}_band_power'] = np.sum(Pxx)

        if ch in SELECTED_FREQ_INDICES:
            features[f'{ch}_entropy'] = energy_entropy_selected(data, SELECTED_FREQ_INDICES[ch], sampling_rate)*3

    return features

# â–¶ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
SAMPLING_RATE = 25600  # 1ì´ˆ = 25600ê°œ

# â¶ ì´ˆ ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹±
def split_into_seconds(df, sampling_rate=25600):
    one_sec = sampling_rate
    num_sec = df.shape[0] // one_sec
    return [df.iloc[i * one_sec : (i + 1) * one_sec] for i in range(num_sec)]

# ---------- ì „ì²´ ë°ì´í„° ë¡œë”© (ë§ˆì§€ë§‰ TDMSëŠ” hold-out) ---------- â˜…ë³€ê²½
def process_all_sets(top_folder, top_n=20, sampling_rate=25600):
    global SELECTED_FREQ_INDICES, FREQ_VECTOR

    rows = []
    pairs = []  # (df, rul, file_name)

    channels = ["CH1", "CH2", "CH3", "CH4"]
    train_folders = sorted(glob(os.path.join(top_folder, "Train*")))

    for set_path in train_folders:
        files = sorted(glob(os.path.join(set_path, "*.tdms")), key=extract_timestamp)
        if not files:
            print(f"âš ï¸ {set_path} í´ë”ì— TDMS íŒŒì¼ ì—†ìŒ")
            continue

        ts_all = [extract_timestamp(f) for f in files]
        end_t = max(ts_all)

        for f, ts in zip(files, ts_all):
            df = load_vibration_data(f)
            if df.empty:
                continue
            rul = (end_t - ts).total_seconds()
            pairs.append((df, rul, os.path.basename(f)))

    # Spearman ê¸°ë°˜ ì£¼íŒŒìˆ˜ ì„ íƒ
    pairs_for_selection = [(df, rul) for df, rul, _ in pairs]
    SELECTED_FREQ_INDICES, FREQ_VECTOR = compute_selected_frequency_indices(
        pairs_for_selection, channels, top_n=top_n, sampling_rate=sampling_rate
    )

    # íŠ¹ì§• ì¶”ì¶œ (1ì´ˆ ë‹¨ìœ„ ìŠ¬ë¼ì´ì‹±)
    for df, rul, fname in pairs:
        for sec_df in split_into_seconds(df, sampling_rate):
            feats = extract_features_from_vibration(sec_df, sampling_rate)
            feats.update({'file': fname, 'RUL': rul})
            rows.append(feats)

    full_df = pd.DataFrame(rows)
    return full_df


# â–¶ ì‹œí€€ìŠ¤ êµ¬ì„± í•¨ìˆ˜
def create_sequences(X, y, window_size=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - window_size + 1):
        X_seq.append(X[i:i+window_size])
        y_seq.append(y[i + window_size - 1])
    return np.array(X_seq), np.array(y_seq)


def compute_percent_error(actual, predicted):
    """ì˜ˆì¸¡ ì˜¤ì°¨ (ë°±ë¶„ìœ¨ %) ê³„ì‚°"""
    actual = np.array(actual)
    predicted = np.array(predicted)
    nonzero_mask = actual != 0
    eri = np.zeros_like(actual)
    eri[nonzero_mask] = 100 * (actual[nonzero_mask] - predicted[nonzero_mask]) / actual[nonzero_mask]
    return eri

def compute_arul_score(eri):
    """ERI (ë°±ë¶„ìœ¨ ì˜¤ì°¨)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ A_RUL ì ìˆ˜ ê³„ì‚°"""
    eri = np.array(eri)
    score = np.where(
        eri <= 0,
        np.exp(-np.log(0.5) * eri / 20),
        np.exp(+np.log(0.5) * eri / 50)
    )
    return score

WINDOW = 5                               # ì‹œí€€ìŠ¤ ê¸¸ì´


if __name__ == "__main__":
    DATA_ROOT = r"c:/Users/ì¡°ì„±ì°¬/OneDrive - UOS/ë°”íƒ• í™”ë©´/ë°°ì–´ë§ë°ì´í„°"
    print("\nğŸ“¦ ì§„ë™ íŠ¹ì§• ì¶”ì¶œ ë° RUL ìƒì„± ì¤‘...")
    
    full_df = process_all_sets(DATA_ROOT)  # holdout_list ì•ˆ ì”€
    full_df = full_df.sort_values(by='file')

    if full_df.empty:
        print("âŒ full_dfê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        exit()

    # â¶ ê³ ì¥ ì§ì „(ìµœì†Œ RUL) íŒŒì¼ â†’ hold-out
    holdout_rows = full_df.groupby(full_df['file'].str.extract(r'(Train\d+)')[0]) \
                          .apply(lambda g: g.loc[g['RUL'].idxmin()]) \
                          .reset_index(drop=True)

    train_val_df = pd.concat([full_df, holdout_rows]).drop_duplicates(keep=False)

    print("\nğŸ§ª ìŠ¤ì¼€ì¼ë§ ë° ì‹œí€€ìŠ¤ êµ¬ì„± ì¤‘...")
    scaler = MinMaxScaler()
    
    # â· stratify ë¼ë²¨ ìƒì„±
    bins   = [-1, 30, 300, 2000, np.inf]
    labels = pd.cut(train_val_df["RUL"], bins=bins, labels=False)

    # ì‹œí€€ìŠ¤ êµ¬ì„±
    X_all = scaler.fit_transform(train_val_df.drop(columns=["RUL", "file"]))
    y_all = train_val_df["RUL"].values
    X_seq, y_seq = create_sequences(X_all, y_all, window_size=5)
    labels_seq   = labels[5-1:].to_numpy()  # WINDOW = 5

    # stratified split
    X_train, X_val, y_train, y_val, _, _ = train_test_split(
        X_seq, y_seq, labels_seq,
        test_size=0.1, random_state=42, stratify=labels_seq
    )
    print(f"\nğŸ“ ì „ì²´ row ìˆ˜: {len(full_df)}ê°œ, ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}ê°œ")
    print("\nğŸ§  LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    model = Sequential([
        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mae')
    model.fit(X_train, y_train, epochs=1000, batch_size=16, validation_data=(X_val, y_val))

    print("\nğŸ“Š ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    pred = model.predict(X_val).flatten()
    actual = y_val

    # ì˜¤ì°¨ ë° í‰ê°€ ì ìˆ˜
    eri = compute_percent_error(actual, pred)
    a_rul = compute_arul_score(eri)

    print(f"\nâœ… í‰ê·  ìƒëŒ€ ì˜¤ì°¨ (MARE): {np.mean(np.abs(eri)):.2f}%")
    print(f"âœ… A_RUL í‰ê·  ì ìˆ˜     : {np.mean(a_rul):.4f}")

    model.save("rul_lstm_all_sets.h5")
    print("\nğŸ’¾ ëª¨ë¸ì´ 'rul_lstm_all_sets.h5'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
