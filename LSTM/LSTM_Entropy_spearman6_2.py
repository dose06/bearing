# rul_model_trainer_all_sets.py (Spearman ê°œì„ : ë‚®ì€ threshold + ìƒìœ„ Nê°œ ì„ íƒ + ë‹¤ì±„ë„)
"""
ğŸ“ rul_model_trainer_all_sets.py

ğŸ¯ ëª©ì :
    - ë‹¤ì±„ë„ ì§„ë™ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë² ì–´ë§ì˜ ë‚¨ì€ ìˆ˜ëª…(RUL, Remaining Useful Life)ì„ ì˜ˆì¸¡í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸(LSTM ê¸°ë°˜)ì„ í•™ìŠµí•˜ê³  í‰ê°€í•¨.

ğŸ“Œ ì£¼ìš” ê¸°ëŠ¥:
    1. Welch ìŠ¤í™íŠ¸ëŸ¼ ë¶„ì„ â†’ Spearman ìƒê´€ê³„ìˆ˜ ê¸°ë°˜ ì¤‘ìš” ì£¼íŒŒìˆ˜ ì„ íƒ
    2. ê³ ì¥ ê´€ë ¨ ì£¼íŒŒìˆ˜ í¬í•¨
    3. ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ (ê°•ì¡°: entropy * 3)
    4. ê° ì±„ë„(CH1~CH4)ì˜ í†µê³„/ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•ì„ ë²¡í„°ë¡œ ë³‘ë ¬ ê²°í•©
    5. 1ì´ˆ ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹±í•œ ë°ì´í„°ì— ëŒ€í•´ ì‹œí€€ìŠ¤ êµ¬ì„±
    6. log1p(RUL) ë³€í™˜ ì ìš©í•˜ì—¬ ì•ˆì •ì  í•™ìŠµ
    7. ê³¼ì†Œì˜ˆì¸¡ì— ë†’ì€ í˜ë„í‹°ë¥¼ ì£¼ëŠ” A_RUL ê·¼ì‚¬ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš© (`approx_arul_loss`)
    8. Stratified split ì ìš© (RUL êµ¬ê°„ ê¸°ì¤€)
    9. í•™ìŠµëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ (MARE, A_RUL)

ğŸ”§ êµ¬ì„± ìš”ì†Œ:
    - Feature ì¶”ì¶œ: mean, std, rms, kurtosis, skew, crest, band_power, entropy
    - ëª¨ë¸ êµ¬ì¡°: LSTM â†’ Dense â†’ Dense(1)
    - ì†ì‹¤ í•¨ìˆ˜: approx_arul_loss (A_RUL ì ìˆ˜ ìµœì í™” ëª©ì )

ğŸ’¾ ì¶œë ¥:
    - rul_lstm_all_sets.h5: í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    - í‰ê·  ìƒëŒ€ ì˜¤ì°¨ (MARE) ë° A_RUL ì ìˆ˜ ì¶œë ¥

ğŸ§ª ì°¸ê³ :
    - log1p, expm1 ë³€í™˜ìœ¼ë¡œ ì•ˆì •ì  ì˜ˆì¸¡ ë° ì—­ë³€í™˜ ìˆ˜í–‰
    - hold-out: ê° Trainì…‹ì˜ ë§ˆì§€ë§‰ TDMS ìƒ˜í”Œì€ í•™ìŠµì— í¬í•¨ë˜ì§€ ì•ŠìŒ
"""
import os
import numpy as np
import pandas as pd
from glob import glob
from scipy.stats import kurtosis, skew, spearmanr
from scipy.signal import welch
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from nptdms import TdmsFile
from tensorflow.keras import backend as K
import tensorflow as tf
from tensorflow.keras import backend as K


# â–¶ Spearman ê¸°ë°˜ ì„ íƒëœ ì£¼íŒŒìˆ˜ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
SELECTED_FREQ_INDICES = {}
FREQ_VECTOR = None

# â–¶ ì§„ë™ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def load_vibration_data(file_path):
    tdms_file = TdmsFile.read(file_path)
    group_name = tdms_file.groups()[0].name  # ì§„ë™ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” ì²« ê·¸ë£¹
    vib_channels = tdms_file[group_name].channels()
    vib_data = {ch.name: ch.data for ch in vib_channels}
    return pd.DataFrame(vib_data)

# â–¶ ì‹œê°„ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
def extract_timestamp(f):
    name = os.path.basename(f)
    time_part = name.split("_")[-1].replace(".tdms", "")
    return pd.to_datetime(time_part, format="%Y%m%d%H%M%S")

FAULT_FREQS = [140, 93, 78]  # ê³ ì¥ ê´€ë ¨ ì£¼íŒŒìˆ˜ (Hz)

def compute_selected_frequency_indices(file_list, channels, top_n=20, sampling_rate=25600):
    psd_by_channel = {ch: [] for ch in channels}
    rul_list = []

    for df, rul in file_list:
        for ch in channels:
            if ch not in df.columns:
                continue
            data = df[ch].values
            f, Pxx = welch(data, fs=sampling_rate, nperseg=4096)
            psd_by_channel[ch].append(Pxx)
        rul_list.append(rul)

    selected = {}
    for ch in channels:
        psd_matrix = np.array(psd_by_channel[ch])
        if psd_matrix.shape[0] == 0:
            continue

        rho_list = [abs(spearmanr(psd_matrix[:, i], rul_list)[0]) for i in range(psd_matrix.shape[1])]
        top_indices = np.argsort(rho_list)[-top_n:]
        fault_indices = [np.argmin(np.abs(f - ff)) for ff in FAULT_FREQS]
        total_indices = sorted(set(top_indices.tolist() + fault_indices))
        selected[ch] = total_indices

    print(f"[âœ“] ì´ˆ ë‹¨ìœ„ Spearman+Fault ê¸°ë°˜ ì£¼íŒŒìˆ˜ ì„ íƒ ì™„ë£Œ (ì´ {len(selected[channels[0]])}ê°œ)")
    return selected, f



# â–¶ ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° í•¨ìˆ˜ (ì„ íƒëœ ì£¼íŒŒìˆ˜ ê¸°ë°˜)
def energy_entropy_selected(data, selected_indices, sampling_rate=25600):
    f, Pxx = welch(data, fs=sampling_rate, nperseg=4096)
    selected = Pxx[selected_indices]
    selected = selected / np.sum(selected)
    selected = selected[selected > 0]
    return -np.sum(selected * np.log(selected))

# â–¶ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
def extract_features_from_vibration(vib_df, sampling_rate=25600):
    features = {}
    for ch in vib_df.columns:
        data = vib_df[ch].values
        rms = np.sqrt(np.mean(data**2))
        f, Pxx = welch(data, fs=sampling_rate, nperseg=4096)

        features[f'{ch}_mean'] = np.mean(data)
        features[f'{ch}_std'] = np.std(data)
        features[f'{ch}_rms'] = rms
        features[f'{ch}_kurtosis'] = kurtosis(data)
        features[f'{ch}_skew'] = skew(data)
        features[f'{ch}_crest'] = np.max(np.abs(data)) / rms
        features[f'{ch}_band_power'] = np.sum(Pxx)

        if ch in SELECTED_FREQ_INDICES:
            features[f'{ch}_entropy'] = energy_entropy_selected(data, SELECTED_FREQ_INDICES[ch], sampling_rate)*3

    return features

# â–¶ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜
SAMPLING_RATE = 25600  # 1ì´ˆ = 25600ê°œ

# â¶ ì´ˆ ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹±
def split_into_seconds(df, sampling_rate=25600):
    one_sec = sampling_rate
    num_sec = df.shape[0] // one_sec
    return [df.iloc[i * one_sec : (i + 1) * one_sec] for i in range(num_sec)]

# ---------- ì „ì²´ ë°ì´í„° ë¡œë”© (ë§ˆì§€ë§‰ TDMSëŠ” hold-out) ---------- â˜…ë³€ê²½
def process_all_sets(top_folder, top_n=20, sampling_rate=25600):
    global SELECTED_FREQ_INDICES, FREQ_VECTOR

    rows = []
    pairs = []  # (df, rul, file_name)

    channels = ["CH1", "CH2", "CH3", "CH4"]
    train_folders = sorted(glob(os.path.join(top_folder, "Train*")))

    for set_path in train_folders:
        files = sorted(glob(os.path.join(set_path, "*.tdms")), key=extract_timestamp)
        if not files:
            print(f"âš ï¸ {set_path} í´ë”ì— TDMS íŒŒì¼ ì—†ìŒ")
            continue

        ts_all = [extract_timestamp(f) for f in files]
        end_t = max(ts_all)

        for f, ts in zip(files[:-1], ts_all[:-1]):
            df = load_vibration_data(f)
            if df.empty:
                continue
            rul = (end_t - ts).total_seconds()
            pairs.append((df, rul, os.path.basename(f)))

    # Spearman ê¸°ë°˜ ì£¼íŒŒìˆ˜ ì„ íƒ
    pairs_for_selection = [(df, rul) for df, rul, _ in pairs]
    SELECTED_FREQ_INDICES, FREQ_VECTOR = compute_selected_frequency_indices(
        pairs_for_selection, channels, top_n=top_n, sampling_rate=sampling_rate
    )

    # íŠ¹ì§• ì¶”ì¶œ (1ì´ˆ ë‹¨ìœ„ ìŠ¬ë¼ì´ì‹±)
    for df, rul, fname in pairs:
        for sec_df in split_into_seconds(df, sampling_rate):
            feats = extract_features_from_vibration(sec_df, sampling_rate)
            feats.update({'file': fname, 'RUL': rul})
            rows.append(feats)

    full_df = pd.DataFrame(rows)
    return full_df

# âœ”ï¸ ê°€ì¤‘ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (log1p ì ìš©í•œ ìƒíƒœì—ì„œ ì‚¬ìš©)
def weighted_mae(y_true, y_pred):
    weights = 1 / (K.clip(y_true, 1, np.inf))  # log1p ëœ ê°’ ê¸°ì¤€
    return K.mean(weights * K.abs(y_true - y_pred))


def approx_arul_loss(y_true, y_pred):
    error = y_true - y_pred
    loss = tf.where(
        error <= 0,
        tf.exp(-K.log(0.5) * error / 20.0),
        tf.exp(K.log(0.5) * error / 50.0)
    )
    return 10000000000 * tf.reduce_mean(loss)  # ğŸ‘‰ ìŠ¤ì¼€ì¼ì„ í‚¤ì›Œì„œ gradient ì¦í­



# â–¶ ì‹œí€€ìŠ¤ êµ¬ì„± í•¨ìˆ˜
def create_sequences(X, y, window_size=5):
    X_seq, y_seq = [], []
    for i in range(len(X) - window_size + 1):
        X_seq.append(X[i:i+window_size])
        y_seq.append(y[i + window_size - 1])
    return np.array(X_seq), np.array(y_seq)


def compute_percent_error(actual, predicted):
    """ì˜ˆì¸¡ ì˜¤ì°¨ (ë°±ë¶„ìœ¨ %) ê³„ì‚°"""
    actual = np.array(actual)
    predicted = np.array(predicted)
    nonzero_mask = actual != 0
    eri = np.zeros_like(actual)
    eri[nonzero_mask] = 100 * (actual[nonzero_mask] - predicted[nonzero_mask]) / actual[nonzero_mask]
    return eri

def compute_arul_score(eri):
    """ERI (ë°±ë¶„ìœ¨ ì˜¤ì°¨)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ A_RUL ì ìˆ˜ ê³„ì‚°"""
    eri = np.array(eri)
    score = np.where(
        eri <= 0,
        np.exp(-np.log(0.5) * eri / 20),
        np.exp(+np.log(0.5) * eri / 50)
    )
    return score

WINDOW = 5                               # ì‹œí€€ìŠ¤ ê¸¸ì´


if __name__ == "__main__":
    DATA_ROOT = r"c:/Users/ì¡°ì„±ì°¬/OneDrive - UOS/ë°”íƒ• í™”ë©´/ë°°ì–´ë§ë°ì´í„°"
    print("\nğŸ“¦ ì§„ë™ íŠ¹ì§• ì¶”ì¶œ ë° RUL ìƒì„± ì¤‘...")
    
    full_df = process_all_sets(DATA_ROOT)  # holdout_list ì•ˆ ì”€
    full_df = full_df.sort_values(by='file')

    if full_df.empty:
        print("âŒ full_dfê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        exit()

    # â¶ ê³ ì¥ ì§ì „(ìµœì†Œ RUL) íŒŒì¼ â†’ hold-out
    holdout_rows = full_df.groupby(full_df['file'].str.extract(r'(Train\d+)')[0]) \
                          .apply(lambda g: g.loc[g['RUL'].idxmin()]) \
                          .reset_index(drop=True)

    train_val_df = pd.concat([full_df, holdout_rows]).drop_duplicates(keep=False)

    print("\nğŸ§ª ìŠ¤ì¼€ì¼ë§ ë° ì‹œí€€ìŠ¤ êµ¬ì„± ì¤‘...")
    scaler = MinMaxScaler()
    
    # â· stratify ë¼ë²¨ ìƒì„±
    bins   = [-1, 30, 300, 2000, np.inf]
    labels = pd.cut(train_val_df["RUL"], bins=bins, labels=False)

    # ì‹œí€€ìŠ¤ êµ¬ì„±
    X_all = scaler.fit_transform(train_val_df.drop(columns=["RUL", "file"]))
    # âœ”ï¸ log1p(RUL) ì ìš©
    y_all_log = np.log1p(train_val_df["RUL"].values)

    # ì‹œí€€ìŠ¤ êµ¬ì„±
    X_seq, y_seq = create_sequences(X_all, y_all_log, window_size=5)

    labels_seq   = labels[5-1:].to_numpy()  # WINDOW = 5

    # stratified split
    X_train, X_val, y_train, y_val, _, _ = train_test_split(
        X_seq, y_seq, labels_seq,
        test_size=0.1, random_state=42, stratify=labels_seq
    )
    print(f"\nğŸ“ ì „ì²´ row ìˆ˜: {len(full_df)}ê°œ, ì‹œí€€ìŠ¤ ìˆ˜: {len(X_seq)}ê°œ")
    print("\nğŸ§  LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    model = Sequential([
        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss= approx_arul_loss)
    model.fit(X_train, y_train, epochs=1000, batch_size=16, validation_data=(X_val, y_val))

    
    print("\nğŸ“Š ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    pred_log = model.predict(X_val)
    pred_log = np.clip(pred_log, 0, 15)  # 15 ë„˜ì–´ê°€ë©´ ìë¦„
    y_val_true = np.expm1(y_val)
    pred = np.expm1(pred_log).flatten()
    actual = y_val_true

    # ì˜¤ì°¨ ë° í‰ê°€ ì ìˆ˜
    eri = compute_percent_error(actual, pred)
    a_rul = compute_arul_score(eri)

    print(f"\nâœ… í‰ê·  ìƒëŒ€ ì˜¤ì°¨ (MARE): {np.mean(np.abs(eri)):.2f}%")
    print(f"âœ… A_RUL í‰ê·  ì ìˆ˜     : {np.mean(a_rul):.4f}")

    model.save("rul_lstm_all_sets.h5")
    print("\nğŸ’¾ ëª¨ë¸ì´ 'rul_lstm_all_sets.h5'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    import matplotlib.pyplot as plt

    plt.scatter(actual, pred, alpha=0.6)
    plt.plot([0, max(actual)], [0, max(actual)], 'r--')  # y=x ê¸°ì¤€ì„ 
    plt.xlabel("True RUL")
    plt.ylabel("Predicted RUL")
    plt.title("RUL ì˜ˆì¸¡ ë¶„í¬")
    plt.grid(True)
    plt.show()